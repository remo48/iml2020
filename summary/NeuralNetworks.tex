\section*{Neural network}
Parameterize feature map: $\phi(x,\theta)$ instead of $\phi(x)$, usually: $\phi(x,\theta) = \varphi(\theta^T x) = \varphi(z)$\\
$\Rightarrow w^* = \underset{w, \theta}{\operatorname{argmin}} \sum_{i=1}^n l(y_i; \sum_{j=1}^m w_j \phi(x_i, \theta_j))$

\subsection*{Activation functions}
Sigmoid: $\frac{1}{1+exp(-z)}$,  $\varphi'(z) = (1 - \varphi(z))\cdot\varphi(z)$\\
Tanh: $\varphi(z) = tanh(z) = \frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}$\\
ReLu:  $\varphi(z) = max(z,0)$

\subsection*{Predict: forward propagation}
$v^{(0)} = x$; for $l = 1,...,L-1$: \\
$v^{(l)} = \varphi(z^{(l)})$, $z^{(l)} = W^{(l)}v^{(l-1)}$\\
$f = W^{(L)}v^{(L-1)}$\\
Predict $f$ for regression, $\operatorname{sign}(f)$ for class.

\subsection*{Compute gradient: backpropagation}
Output layer: 
$\delta_j = l_j'(f_j)$,
$\frac{\partial}{\partial w_{j,i}} = \delta_j v_i$\\
Hidden layer $l=L-1,...,1$:\\
$\delta_j = \varphi'(z_j) \cdot \sum_{i\in Layer_{l+1}} w_{i,j}\delta_i$,
$\frac{\partial}{\partial w_{j,i}} = \delta_j v_i$

\subsection*{Learning with momentum}
$a \leftarrow m \cdot a + \eta_t \nabla_W l(W;y,x)$; $W \leftarrow W - a$