{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "main2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJCSx67y27Bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.data import Dataset\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "# path to folder in google drive, where task4_handout.zip is located\n",
        "BASE_FOLDER = '/content/drive/My Drive/development/machine_learning/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGOyW0lJ3G3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_drive():\n",
        "  from google.colab import drive\n",
        "  import sys\n",
        "  from pathlib import Path\n",
        "  drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "  # path to base folder \n",
        "  base = Path(BASE_FOLDER)\n",
        "  sys.path.append(str(base))\n",
        "\n",
        "  zip_path = base/'task4_handout.zip'\n",
        "\n",
        "  # unzip handout\n",
        "  !cp '{zip_path}' .\n",
        "  !unzip -q task4_handout.zip\n",
        "  !rm task4_handout.zip\n",
        "\n",
        "  # unzip image data\n",
        "  !unzip -q food.zip\n",
        "  !rm food.zip\n",
        "  !rm food/.DS_Store"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0o9JT1H27Bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_count(fname):\n",
        "  return sum(1 for line in open(fname))\n",
        "        \n",
        "def euclidean_distance(x, y):\n",
        "  return tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
        "\n",
        "def triplet_loss(y_true, y_pred):\n",
        "  \"\"\"custom loss function to reduce the distance of anchor and positive embeddings, while increasing the \n",
        "     distance of anchor and negative embeddings \n",
        "  \"\"\"\n",
        "  del y_true\n",
        "  alpha = 0.2\n",
        "\n",
        "  anchor = y_pred[:,0]\n",
        "  positive = y_pred[:,1]\n",
        "  negative = y_pred[:,2]\n",
        "\n",
        "  pos_dist = euclidean_distance(anchor, positive)\n",
        "  neg_dist = euclidean_distance(anchor, negative)\n",
        "  ter_dist = euclidean_distance(positive, negative)\n",
        "\n",
        "  basic_loss = pos_dist-.6*neg_dist -.4*ter_dist+alpha\n",
        "  loss = tf.reduce_mean(tf.maximum(basic_loss,0))\n",
        "\n",
        "  return loss\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  \"\"\"custom accuracy function defined as fraction of properly classified triplets\n",
        "  \"\"\"\n",
        "  anchor = y_pred[:,0]\n",
        "  positive = y_pred[:,1]\n",
        "  negative = y_pred[:,2]\n",
        "\n",
        "  pos_dist = euclidean_distance(anchor, positive)\n",
        "  neg_dist = euclidean_distance(anchor, negative)\n",
        "\n",
        "  return tf.reduce_mean(tf.cast(tf.greater_equal(neg_dist, pos_dist), tf.float32))\n",
        "\n",
        "def prepare_data(triplets, validation_size=0.3):\n",
        "  \"\"\"create disjoint train and validation triplets\n",
        "  \"\"\"\n",
        "  def check_disjoint(a, b):\n",
        "    return set(a).isdisjoint(b)\n",
        "\n",
        "  with open(triplets, 'r') as fin, open('train.txt', 'w') as foutTrain, open('validation.txt', 'w') as foutVal:\n",
        "    lines = fin.readlines()\n",
        "    imgset = [item for line in lines for item in line.split()]\n",
        "    imgset = list(dict.fromkeys(imgset))\n",
        "\n",
        "    train_lines, val_lines = train_test_split(imgset, test_size=validation_size, random_state=42)\n",
        "\n",
        "    for line in lines:\n",
        "      if check_disjoint(line.split(), val_lines):\n",
        "        foutTrain.write(line)\n",
        "\n",
        "      elif check_disjoint(line.split(), train_lines):\n",
        "        foutVal.write(line)\n",
        "      \n",
        "\n",
        "def augment_triplet(triplet, label):\n",
        "  \"\"\"data augmentation for triplets\n",
        "  \"\"\"\n",
        "  def augment_image(image):\n",
        "    image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_saturation(image, 0.5, 1.5)\n",
        "    image = tf.image.random_brightness(image, 32.0/255.0)\n",
        "    image = tf.image.random_hue(image, 32.0/255.0)\n",
        "    image = tf.image.random_contrast(image, 0.7, 1.3)\n",
        "    return image\n",
        "    \n",
        "  a, p, n = triplet\n",
        "  return (augment_image(a), augment_image(p), augment_image(n)), label\n",
        "\n",
        "\n",
        "def load_triplet(triplet):\n",
        "\n",
        "  triplet = tf.strings.split(triplet)\n",
        "\n",
        "  anchor = load_image(triplet[0])\n",
        "  positive = load_image(triplet[1])\n",
        "  negative = load_image(triplet[2])\n",
        "\n",
        "  return (anchor, positive, negative), 1\n",
        "\n",
        "def create_dataset(triplets):\n",
        "  \"\"\"creates a tf.data.Dataframe based on a triplet text file\n",
        "  \"\"\"\n",
        "  def load_image(img):\n",
        "    filename = 'food/'+img+'.jpg'\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = preprocess_input(image)\n",
        "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    return image\n",
        "  \n",
        "  def load_triplet(triplet):\n",
        "    triplet = tf.strings.split(triplet)\n",
        "\n",
        "    anchor = load_image(triplet[0])\n",
        "    positive = load_image(triplet[1])\n",
        "    negative = load_image(triplet[2])\n",
        "\n",
        "    return (anchor, positive, negative), 1\n",
        "\n",
        "  content = tf.data.TextLineDataset(triplets)\n",
        "  ds = content.map(load_triplet, num_parallel_calls=AUTOTUNE)\n",
        "  return ds\n",
        "  \n",
        "def show(image):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "id": "Ncv9-qGc27B2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(emb_size):\n",
        "  base_model = MobileNetV2(input_shape=(IMG_HEIGHT,IMG_WIDTH,3), include_top=False, weights='imagenet')\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(emb_size)(x)\n",
        "  x = layers.Lambda(lambda vect: tf.math.l2_normalize(vect, axis=1))(x)\n",
        "\n",
        "  encoder = Model(base_model.input, x)\n",
        "\n",
        "  inA = layers.Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
        "  inB = layers.Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
        "  inC = layers.Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
        "\n",
        "  encA = encoder(inA)\n",
        "  encB = encoder(inB)\n",
        "  encC = encoder(inC)\n",
        "\n",
        "  stacked = layers.Lambda(lambda vects: tf.stack(vects, axis=1))([encA, encB, encC])\n",
        "  model = Model((inA, inB, inC), stacked)\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_classifier(model):\n",
        "  def classify(encoded):\n",
        "    a = encoded[:,0]\n",
        "    b = encoded[:,1]\n",
        "    c = encoded[:,2]\n",
        "\n",
        "    pos_dist = euclidean_distance(a, b)\n",
        "    neg_dist = euclidean_distance(a, c)\n",
        "\n",
        "    return tf.cast((tf.greater_equal(neg_dist, pos_dist)), tf.float32)\n",
        "\n",
        "  x = layers.Lambda(classify)(model.output)\n",
        "  classifier = Model(model.input, x)\n",
        "  \n",
        "  return classifier"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6aatPFqD-l9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "5d69c20a-65dd-4dfe-e974-abc465b35adc"
      },
      "source": [
        "# --------------------\n",
        "# data preparation\n",
        "# --------------------\n",
        "\n",
        "load_drive()\n",
        "\n",
        "prepare_data('train_triplets.txt', validation_size=0.25)\n",
        "triplets_train = file_count('train.txt')\n",
        "triplets_validate = file_count('validation.txt')\n",
        "print('train triplets:', triplets_train, '\\nvalidate triplets:',triplets_validate, '\\ntotal:', triplets_train+triplets_validate)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "train triplets: 25378 \n",
            "validate triplets: 858 \n",
            "total: 26236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqgP_lzA27B9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a6c2fd01-e2e8-42b7-902f-a47f29fefbc0"
      },
      "source": [
        "# --------------------\n",
        "# train\n",
        "# --------------------\n",
        "\n",
        "train_batch_size = 32\n",
        "validation_batch_size = 128\n",
        "num_epochs = 6\n",
        "train_len = file_count('train.txt')\n",
        "validation_len = file_count('validation.txt')\n",
        "train_steps = int(np.ceil(train_len / float(train_batch_size)))\n",
        "validation_steps = int(np.ceil(validation_len / float(validation_batch_size)))\n",
        "\n",
        "train_ds = create_dataset('train.txt')\n",
        "train_ds = (train_ds\n",
        "    .repeat()\n",
        "    .shuffle(buffer_size=1000)\n",
        "    .map(augment_triplet, num_parallel_calls=AUTOTUNE) # augment image data to prevent overfitting\n",
        "    .batch(train_batch_size)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "validation_ds = create_dataset('validation.txt')\n",
        "validation_ds = (validation_ds\n",
        "    .repeat()\n",
        "    .batch(validation_batch_size)\n",
        ")\n",
        "\n",
        "\n",
        "model = create_model(128)\n",
        "model.compile(optimizer=Adam(lr=0.0005), loss=triplet_loss, metrics=[accuracy])\n",
        "\n",
        "hist = model.fit(train_ds,\n",
        "                 steps_per_epoch=train_steps,\n",
        "                 epochs=num_epochs, \n",
        "                 validation_data=validation_ds,\n",
        "                 validation_steps=validation_steps\n",
        "                 )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "794/794 [==============================] - 367s 462ms/step - loss: 0.1404 - accuracy: 0.6883 - val_loss: 0.1535 - val_accuracy: 0.6763\n",
            "Epoch 2/2\n",
            "794/794 [==============================] - 375s 472ms/step - loss: 0.1387 - accuracy: 0.6893 - val_loss: 0.1493 - val_accuracy: 0.6797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxwZRyrRGkzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1e63420-01cd-4baa-96b2-0c799a4e4f00"
      },
      "source": [
        "# --------------------\n",
        "# predict\n",
        "# --------------------\n",
        "\n",
        "test_steps = int(np.ceil(file_count('test_triplets.txt')/128.0))\n",
        "test_ds = create_dataset('test_triplets.txt').batch(128)\n",
        "classifier = create_classifier(model)\n",
        "y_pred = classifier.predict(test_ds, verbose=1, steps=test_steps)\n",
        "np.savetxt('submission.txt', y_pred, fmt='%d')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "466/466 [==============================] - 544s 1s/step\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}